{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fcb11bde920>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HookedTransformer.from_pretrained('gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 51200-L1-8e-05-LR-0.0004-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.002048\n",
      "Total training steps: 488\n",
      "Total wandb updates: 4\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 2621.44\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "from sae_lens.training.sparse_autoencoder import SparseAutoencoder\n",
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "\n",
    "hook_point = \"blocks.20.hook_resid_pre\"\n",
    "bs = 64\n",
    "\n",
    "cf = {\n",
    "  \"model_name\": \"gpt2-xl\",\n",
    "  \"hook_point\": \"blocks.20.hook_resid_pre\",\n",
    "  \"hook_point_layer\": 20,\n",
    "  \"hook_point_head_index\": None,\n",
    "  \"dataset_path\": \"Skylion007/openwebtext\",\n",
    "  \"is_dataset_tokenized\": False,\n",
    "  \"context_size\": 128,\n",
    "  \"use_cached_activations\": False,\n",
    "  \"cached_activations_path\": \"activations/Skylion007_openwebtext/gpt2-small/blocks.1.hook_resid_pre\",\n",
    "  \"d_in\": 1600,\n",
    "  \"n_batches_in_buffer\": bs,\n",
    "  \"total_training_tokens\": 300000000,\n",
    "  \"store_batch_size\": bs,\n",
    "  \"device\": device,\n",
    "  \"seed\": 42,\n",
    "  \"dtype\": \"torch.float16\",\n",
    "  \"b_dec_init_method\": \"geometric_median\",\n",
    "  \"expansion_factor\": 32,\n",
    "  \"from_pretrained_path\": None,\n",
    "  \"l1_coefficient\": 0.00008,\n",
    "  \"lr\": 0.0004,\n",
    "  \"lr_scheduler_name\": None,\n",
    "  \"lr_warm_up_steps\": 5000,\n",
    "  \"train_batch_size\": 4096,\n",
    "  \"use_ghost_grads\": False,\n",
    "  \"feature_sampling_window\": 1000,\n",
    "  \"feature_sampling_method\": None,\n",
    "  \"resample_batches\": 1028,\n",
    "  \"feature_reinit_scale\": 0.2,\n",
    "  \"dead_feature_window\": 5000,\n",
    "  \"dead_feature_estimation_method\": \"no_fire\",\n",
    "  \"dead_feature_threshold\": 1e-8,\n",
    "  \"log_to_wandb\": True,\n",
    "  \"wandb_project\": \"mats_sae_training_gpt2_small_resid_pre_5\",\n",
    "  \"wandb_entity\": None,\n",
    "  \"wandb_log_frequency\": 100,\n",
    "  \"n_checkpoints\": 10,\n",
    "  \"checkpoint_path\": \"checkpoints/mm179kd2\",\n",
    "  \"d_sae\": 1600*32,\n",
    "  \"tokens_per_buffer\": 128,\n",
    "  \"run_name\": \"24576-L1-8e-05-LR-0.0004-Tokens-3.000e+08\"\n",
    "}\n",
    "config = cf\n",
    "var_names = LanguageModelSAERunnerConfig.__init__.__code__.co_varnames\n",
    "config = {k: v for k, v in config.items() if k in var_names}\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    **config\n",
    ")\n",
    "sparse_autoencoder = SparseAutoencoder(cfg)\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device\n",
    "\n",
    "layer = cfg.hook_point_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "tensors = {}\n",
    "with safe_open(\"gpt2-20.safetensors\", framework=\"pt\") as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k).cuda()\n",
    "        # if k == \"b_enc\":\n",
    "        #     tensors[k] -= 0.18\n",
    "tensors[\"b_enc\"] += tensors[\"b_dec\"] @ tensors[\"W_enc\"]\n",
    "sparse_autoencoder.load_state_dict(tensors, strict=False)\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50.7812, 49.5312, 26.0781, 20.3281, 20.2812, 19.3594, 18.5000, 16.7812,\n",
       "         16.3906, 13.1094], device='cuda:0', dtype=torch.float16),\n",
       " tensor([49395,   126,   409, 39324, 44188, 14643, 22759,  1104,  5883,  5913],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_acts_at_pos(text, pos=-1, silent=True, prepend_bos=True, n_top=10):\n",
    "    logits, cache = model.run_with_cache(text, prepend_bos=prepend_bos)\n",
    "    if pos is None:\n",
    "        hidden_state = cache[hook_point][0, :, :]\n",
    "    else:\n",
    "        hidden_state = cache[hook_point][0, pos, :].unsqueeze(0)\n",
    "    feature_acts = sparse_autoencoder(hidden_state).feature_acts\n",
    "    feature_acts = feature_acts.mean(dim=0)\n",
    "    top_v, top_i = torch.topk(feature_acts, n_top)\n",
    "    return top_v, top_i\n",
    "\n",
    "top_acts_at_pos(\"Anger something something\", pos=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([65.2500, 43.9062, 26.2188, 23.8750, 23.5938, 21.7500, 19.1719, 16.3906,\n",
       "         15.8984, 13.6094], device='cuda:0', dtype=torch.float16),\n",
       " tensor([  126, 20811,   409, 44188,  4524,  4364, 12006, 33085, 22759, 25116],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acts_at_pos(\"Anger\", pos=-1) # [126, 20811, 4524 ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "# data = load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "\n",
    "# tokenized_data = tutils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "# tokenized_data = tokenized_data.shuffle(42)\n",
    "# all_tokens = tokenized_data[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 51200-L1-8e-05-LR-0.0004-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.002048\n",
      "Total training steps: 488\n",
      "Total wandb updates: 4\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 2621.44\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Run name: 51200-L1-8e-05-LR-0.0004-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.002048\n",
      "Total training steps: 488\n",
      "Total wandb updates: 4\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 2621.44\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7acc411f36c44b9b63c46c227d0fdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
    "import transformer_lens.loading_from_pretrained as tllfp\n",
    "loader = LMSparseAutoencoderSessionloader(sparse_autoencoder.cfg)\n",
    "_, _, activation_store = loader.load_sae_training_group_session()\n",
    "\n",
    "\n",
    "def get_tokens(\n",
    "    activation_store,\n",
    "    n_batches_to_sample_from: int = 2**13,\n",
    "    n_prompts_to_select: int = 4096 * 6,\n",
    "):\n",
    "    all_tokens_list = []\n",
    "    pbar = tqdm(range(n_batches_to_sample_from))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activation_store.get_batch_tokens()\n",
    "        batch_tokens = batch_tokens[torch.randperm(batch_tokens.shape[0])][\n",
    "            : batch_tokens.shape[0]\n",
    "        ]\n",
    "        all_tokens_list.append(batch_tokens)\n",
    "\n",
    "    all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "    all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "    return all_tokens[:n_prompts_to_select]\n",
    "\n",
    "\n",
    "all_tokens = get_tokens(activation_store)  # should take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24576, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98984375\n"
     ]
    }
   ],
   "source": [
    "# okay so 126 is the anger feature right? Wrong!\n",
    "# 126 activates on most text\n",
    "selected_feature = 126\n",
    "activation_count = 0\n",
    "total = 0\n",
    "for i in range(10):\n",
    "    logits, cache = model.run_with_cache(all_tokens[i])\n",
    "    hidden_state = cache[hook_point][0]\n",
    "    feature_acts = sparse_autoencoder(hidden_state).feature_acts # shape [128, n_features]\n",
    "    selected_acts = feature_acts[:, selected_feature]\n",
    "\n",
    "    activation_count += (selected_acts > 0).sum().item()\n",
    "    total += selected_acts.shape[0]\n",
    "\n",
    "print(activation_count/total)\n",
    "# this feature activates on 99% of all tokens!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3b8bed953943d0b144350babfccd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2283dc41c36b4bfebd2928928d88cdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time  </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 0.67s │ 46.8% │\n",
       "│ (3) Computing feature acts from model acts     │ 0.29s │ 20.5% │\n",
       "│ (4) Getting data for tables                    │ 0.00s │ 0.1%  │\n",
       "│ (5) Getting data for histograms                │ 0.10s │ 7.3%  │\n",
       "│ (6) Getting data for sequences                 │ 0.10s │ 6.8%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.26s │ 18.5% │\n",
       "└────────────────────────────────────────────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 0.67s │ 46.8% │\n",
       "│ (3) Computing feature acts from model acts     │ 0.29s │ 20.5% │\n",
       "│ (4) Getting data for tables                    │ 0.00s │ 0.1%  │\n",
       "│ (5) Getting data for histograms                │ 0.10s │ 7.3%  │\n",
       "│ (6) Getting data for sequences                 │ 0.10s │ 6.8%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.26s │ 18.5% │\n",
       "└────────────────────────────────────────────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "test_feature_idx_gpt = [126, 20811, 409]\n",
    "\n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point=hook_point,\n",
    "    features=test_feature_idx_gpt,\n",
    "    batch_size=bs,\n",
    "    minibatch_size_tokens=128,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    sae_vis_data_gpt = SaeVisData.create(\n",
    "        encoder=sparse_autoencoder,\n",
    "        model=model,\n",
    "        tokens=all_tokens,  # type: ignore\n",
    "        cfg=feature_vis_config_gpt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8f401dd93f4e1fbe608d53bf60caf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/126_feature_vis.html' target='_blank'>feature_vis/126_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/126_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678cf7d320604accb9e269aa53e985a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/20811_feature_vis.html' target='_blank'>feature_vis/20811_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/20811_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d640863568342f0b9ad36eb52d340ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/409_feature_vis.html' target='_blank'>feature_vis/409_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/409_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "vis_dir = \"feature_vis\"\n",
    "if not os.path.exists(vis_dir):\n",
    "    os.makedirs(vis_dir)\n",
    "\n",
    "for idx, feature in enumerate(test_feature_idx_gpt):\n",
    "    if sae_vis_data_gpt.feature_stats.max[idx] == 0:\n",
    "        continue\n",
    "    filename = os.path.join(vis_dir, f\"{feature}_feature_vis.html\")\n",
    "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
    "    display(FileLink(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([60.2188, 43.5312, 25.0625, 16.5625, 16.2969, 12.7969, 12.0156, 10.2422,\n",
       "          9.4375,  9.1172], device='cuda:0', dtype=torch.float16),\n",
       " tensor([  126, 41137,   409, 28732, 44188, 22759,  5883, 43778, 35851, 18118],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acts_at_pos(\" any text you like!\", pos=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([652.5000, 517.0000, 501.7500, 490.7500, 485.2500, 473.5000, 466.2500,\n",
       "         450.2500, 415.7500, 377.7500], device='cuda:0', dtype=torch.float16),\n",
       " tensor([30958, 47390, 41028,  1212, 40249,  3370,  9507, 10284, 30590, 17048],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also, another crazy thing about this SAE:\n",
    "# the 0th token causes some features to activate with crazy high magnitude\n",
    "top_acts_at_pos(\" hello\", pos=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([616.5000, 490.0000, 475.0000, 460.2500, 446.5000, 445.5000, 437.5000,\n",
       "         420.0000, 388.2500, 376.2500], device='cuda:0', dtype=torch.float16),\n",
       " tensor([30958, 47390,  1212, 41028, 40249,  9507,  3370, 10284, 30590, 42322],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing with and without BOS\n",
    "top_acts_at_pos(\" hello\", pos=0, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my takeaway is this is a bad SAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that you can indeed find a reasonable \"Angry\" feature.\n",
    "# get features for angry, remove features which also activate on \"Calm\"\n",
    "\n",
    "angry_vals, angry_ids = top_acts_at_pos(\"Anger\", pos=-1, n_top=100)\n",
    "calm_vals, calm_ids = top_acts_at_pos(\"Calm\", pos=None, n_top=200)\n",
    "\n",
    "angry_vals = [t.item() for t in angry_vals]\n",
    "angry_ids = [t.item() for t in angry_ids]\n",
    "calm_vals = [t.item() for t in calm_vals]\n",
    "calm_ids = [t.item() for t in calm_ids]\n",
    "\n",
    "angry = zip(angry_vals, angry_ids)\n",
    "calm = zip(calm_vals, calm_ids)\n",
    "\n",
    "# remove zero-valued calm ids\n",
    "calm = [(v, i) for v, i in calm if v > 0]\n",
    "calm_set = set([i for v, i in calm])\n",
    "\n",
    "# remove calm ids from angry\n",
    "angry = [(v, i) for v, i in angry if i not in calm_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(43.90625, 20811),\n",
       " (21.75, 4364),\n",
       " (16.390625, 33085),\n",
       " (11.4921875, 25473),\n",
       " (10.84375, 34590),\n",
       " (10.1796875, 11977),\n",
       " (8.78125, 21255),\n",
       " (8.1171875, 12346),\n",
       " (6.46875, 21981),\n",
       " (5.80859375, 37635)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angry[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/20811_feature_vis.html' target='_blank'>feature_vis/20811_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/20811_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/4364_feature_vis.html' target='_blank'>feature_vis/4364_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/4364_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/33085_feature_vis.html' target='_blank'>feature_vis/33085_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/33085_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_feature_idx_gpt = [20811, 4364, 33085]\n",
    "\n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point=hook_point,\n",
    "    features=test_feature_idx_gpt,\n",
    "    batch_size=bs,\n",
    "    minibatch_size_tokens=128,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    sae_vis_data_gpt = SaeVisData.create(\n",
    "        encoder=sparse_autoencoder,\n",
    "        model=model,\n",
    "        tokens=all_tokens,  # type: ignore\n",
    "        cfg=feature_vis_config_gpt,\n",
    "    )\n",
    "\n",
    "for idx, feature in enumerate(test_feature_idx_gpt):\n",
    "    if sae_vis_data_gpt.feature_stats.max[idx] == 0:\n",
    "        continue\n",
    "    filename = os.path.join(vis_dir, f\"{feature}_feature_vis.html\")\n",
    "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
    "    display(FileLink(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20811 # angry/sad/irritated feature\n",
    "# 4364 activates on words that end in \"er\" or \"ner\"\n",
    "# 33085 violence/aggression feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
