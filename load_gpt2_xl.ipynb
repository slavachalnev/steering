{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HookedTransformer.from_pretrained('gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 51200-L1-8e-05-LR-0.0004-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.002048\n",
      "Total training steps: 488\n",
      "Total wandb updates: 4\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 2621.44\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "\n",
    "hook_point = \"blocks.20.hook_resid_pre\"\n",
    "bs = 64\n",
    "\n",
    "cf = {\n",
    "  \"model_name\": \"gpt2-xl\",\n",
    "  \"hook_point\": \"blocks.20.hook_resid_pre\",\n",
    "  \"hook_point_layer\": 20,\n",
    "  \"hook_point_head_index\": None,\n",
    "  \"dataset_path\": \"Skylion007/openwebtext\",\n",
    "  \"is_dataset_tokenized\": False,\n",
    "  \"context_size\": 128,\n",
    "  \"use_cached_activations\": False,\n",
    "  \"cached_activations_path\": \"activations/Skylion007_openwebtext/gpt2-small/blocks.1.hook_resid_pre\",\n",
    "  \"d_in\": 1600,\n",
    "  \"n_batches_in_buffer\": bs,\n",
    "  \"total_training_tokens\": 300000000,\n",
    "  \"store_batch_size\": bs,\n",
    "  \"device\": device,\n",
    "  \"seed\": 42,\n",
    "  \"dtype\": \"torch.float16\",\n",
    "  \"b_dec_init_method\": \"geometric_median\",\n",
    "  \"expansion_factor\": 32,\n",
    "  \"from_pretrained_path\": None,\n",
    "  \"l1_coefficient\": 0.00008,\n",
    "  \"lr\": 0.0004,\n",
    "  \"lr_scheduler_name\": None,\n",
    "  \"lr_warm_up_steps\": 5000,\n",
    "  \"train_batch_size\": 4096,\n",
    "  \"use_ghost_grads\": False,\n",
    "  \"feature_sampling_window\": 1000,\n",
    "  \"feature_sampling_method\": None,\n",
    "  \"resample_batches\": 1028,\n",
    "  \"feature_reinit_scale\": 0.2,\n",
    "  \"dead_feature_window\": 5000,\n",
    "  \"dead_feature_estimation_method\": \"no_fire\",\n",
    "  \"dead_feature_threshold\": 1e-8,\n",
    "  \"log_to_wandb\": True,\n",
    "  \"wandb_project\": \"mats_sae_training_gpt2_small_resid_pre_5\",\n",
    "  \"wandb_entity\": None,\n",
    "  \"wandb_log_frequency\": 100,\n",
    "  \"n_checkpoints\": 10,\n",
    "  \"checkpoint_path\": \"checkpoints/mm179kd2\",\n",
    "  \"d_sae\": 1600*32,\n",
    "  \"tokens_per_buffer\": 128,\n",
    "  \"run_name\": \"24576-L1-8e-05-LR-0.0004-Tokens-3.000e+08\"\n",
    "}\n",
    "config = cf\n",
    "var_names = LanguageModelSAERunnerConfig.__init__.__code__.co_varnames\n",
    "config = {k: v for k, v in config.items() if k in var_names}\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    **config\n",
    ")\n",
    "sparse_autoencoder = SparseAutoencoder(cfg)\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device\n",
    "\n",
    "layer = cfg.hook_point_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "tensors = {}\n",
    "with safe_open(\"gpt2-20.safetensors\", framework=\"pt\") as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k).cuda()\n",
    "        # if k == \"b_enc\":\n",
    "        #     tensors[k] -= 0.18\n",
    "tensors[\"b_enc\"] += tensors[\"b_dec\"] @ tensors[\"W_enc\"]\n",
    "sparse_autoencoder.load_state_dict(tensors, strict=False)\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([55.3438, 49.4688, 25.2812, 22.7031, 22.5625, 21.6250, 20.6094, 19.8750,\n",
       "         16.4375, 15.4297], device='cuda:0', dtype=torch.float16,\n",
       "        grad_fn=<TopkBackward0>),\n",
       " tensor([49395,   126,   409, 14643, 44188, 39324,  1104, 22759,  5883,  5913],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_acts_at_pos(text, pos=-1, silent=True, prepend_bos=True):\n",
    "    logits, cache = model.run_with_cache(text, prepend_bos=prepend_bos)\n",
    "    if pos is None:\n",
    "        hidden_state = cache[hook_point][0, :, :]\n",
    "    else:\n",
    "        hidden_state = cache[hook_point][0, pos, :].unsqueeze(0)\n",
    "    feature_acts = sparse_autoencoder(hidden_state).feature_acts\n",
    "    print(feature_acts.shape)\n",
    "    feature_acts = feature_acts.mean(dim=0)\n",
    "    top_v, top_i = torch.topk(feature_acts, 10)\n",
    "    return top_v, top_i\n",
    "\n",
    "top_acts_at_pos(\"Anger something something\", pos=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([65.1875, 47.1250, 26.8281, 26.1562, 25.6562, 25.4219, 22.2188, 19.3125,\n",
       "         18.8438, 17.2656], device='cuda:0', dtype=torch.float16,\n",
       "        grad_fn=<TopkBackward0>),\n",
       " tensor([  126, 20811,  4524, 44188,  4364,   409, 12006, 33085, 25116, 22759],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acts_at_pos(\"Anger\", pos=-1) # [126, 20811, 4524 ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "\n",
    "tokenized_data = tutils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([325017, 128])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9921875\n"
     ]
    }
   ],
   "source": [
    "# okay so 126 is the anger feature right? Wrong!\n",
    "# 126 activates on most text\n",
    "selected_feature = 126\n",
    "activation_count = 0\n",
    "total = 0\n",
    "for i in range(10):\n",
    "    logits, cache = model.run_with_cache(all_tokens[i])\n",
    "    hidden_state = cache[hook_point][0]\n",
    "    feature_acts = sparse_autoencoder(hidden_state).feature_acts # shape [128, n_features]\n",
    "    selected_acts = feature_acts[:, selected_feature]\n",
    "\n",
    "    activation_count += (selected_acts > 0).sum().item()\n",
    "    total += selected_acts.shape[0]\n",
    "\n",
    "print(activation_count/total)\n",
    "# this feature activates on 99% of all tokens!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c83a0182b645f59fb3ba4cc5ffd7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe41bb6e8694a7ab22c2a0ae45e82ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time  </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 0.68s │ 57.7% │\n",
       "│ (3) Computing feature acts from model acts     │ 0.33s │ 27.7% │\n",
       "│ (4) Getting data for tables                    │ 0.00s │ 0.1%  │\n",
       "│ (5) Getting data for histograms                │ 0.10s │ 8.3%  │\n",
       "│ (6) Getting data for sequences                 │ 0.06s │ 5.1%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.01s │ 1.0%  │\n",
       "└────────────────────────────────────────────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 0.68s │ 57.7% │\n",
       "│ (3) Computing feature acts from model acts     │ 0.33s │ 27.7% │\n",
       "│ (4) Getting data for tables                    │ 0.00s │ 0.1%  │\n",
       "│ (5) Getting data for histograms                │ 0.10s │ 8.3%  │\n",
       "│ (6) Getting data for sequences                 │ 0.06s │ 5.1%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.01s │ 1.0%  │\n",
       "└────────────────────────────────────────────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "test_feature_idx_gpt = [126, 20811, 409]\n",
    "\n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point=hook_point,\n",
    "    features=test_feature_idx_gpt,\n",
    "    batch_size=bs,\n",
    "    minibatch_size_tokens=128,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    sae_vis_data_gpt = SaeVisData.create(\n",
    "        encoder=sparse_autoencoder,\n",
    "        model=model,\n",
    "        tokens=all_tokens,  # type: ignore\n",
    "        cfg=feature_vis_config_gpt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6602daeead82404bb431da8ee6cb0055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/126_feature_vis.html' target='_blank'>feature_vis/126_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/126_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ead6a0497849f68288593d66d00814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/20811_feature_vis.html' target='_blank'>feature_vis/20811_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/20811_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ce2a31e4484a279ab24c9d5be1eb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='feature_vis/409_feature_vis.html' target='_blank'>feature_vis/409_feature_vis.html</a><br>"
      ],
      "text/plain": [
       "/home/slava/safety/steering/feature_vis/409_feature_vis.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "vis_dir = \"feature_vis\"\n",
    "if not os.path.exists(vis_dir):\n",
    "    os.makedirs(vis_dir)\n",
    "\n",
    "for idx, feature in enumerate(test_feature_idx_gpt):\n",
    "    if sae_vis_data_gpt.feature_stats.max[idx] == 0:\n",
    "        continue\n",
    "    filename = os.path.join(vis_dir, f\"{feature}_feature_vis.html\")\n",
    "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
    "    display(FileLink(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([60.1562, 48.0938, 24.2656, 20.2188, 18.5781, 14.2656, 14.1641, 12.8672,\n",
       "         12.0781, 11.9766], device='cuda:0', dtype=torch.float16,\n",
       "        grad_fn=<TopkBackward0>),\n",
       " tensor([  126, 41137,   409, 28732, 44188, 35851, 22759, 43778,  5883, 18118],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_acts_at_pos(\" any text you like!\", pos=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([656.0000, 518.5000, 505.7500, 494.5000, 487.0000, 478.0000, 468.7500,\n",
       "         453.5000, 419.0000, 385.0000], device='cuda:0', dtype=torch.float16,\n",
       "        grad_fn=<TopkBackward0>),\n",
       " tensor([30958, 47390, 41028,  1212, 40249,  3370,  9507, 10284, 30590, 42322],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also, another crazy thing about this SAE:\n",
    "# the 0th token causes some features to activate with crazy high magnitude\n",
    "top_acts_at_pos(\" hello\", pos=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([620.0000, 491.2500, 478.7500, 464.2500, 448.2500, 448.0000, 442.0000,\n",
       "         423.2500, 391.5000, 383.7500], device='cuda:0', dtype=torch.float16,\n",
       "        grad_fn=<TopkBackward0>),\n",
       " tensor([30958, 47390,  1212, 41028, 40249,  9507,  3370, 10284, 30590, 42322],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing with and without BOS\n",
    "top_acts_at_pos(\" hello\", pos=0, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my takeaway is this is a bad SAE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
