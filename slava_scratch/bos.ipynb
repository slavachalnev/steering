{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa20113ffa0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sae_lens import SparseAutoencoder\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.eval_utils import evaluate_completions\n",
    "from steering.utils import text_to_sae_feats, top_activations, normalise_decoder, get_activation_steering\n",
    "from steering.patch import generate, get_scores_and_losses, get_loss, scores_2d, scores_clamp_2d\n",
    "\n",
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e48f912e4be410c8920f0dfe655f45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_resid(resid, hook, steering, c=1, pos=None):\n",
    "    assert len(steering.shape) == 3 # [batch_size, sequence_length, d_model]\n",
    "\n",
    "    if pos is None:\n",
    "        # insert at all positions\n",
    "        assert steering.shape[1] == 1\n",
    "        resid[:, :, :] = resid[:, :, :] + c * steering\n",
    "        return resid\n",
    "\n",
    "    n_toks = min(resid.shape[1] - pos, steering.shape[1])\n",
    "    if pos < resid.shape[1]:\n",
    "        resid[:, pos:n_toks+pos, :] = resid[:, pos:n_toks+pos, :] + c * steering[:, :n_toks, :]\n",
    "    \n",
    "    return resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp6 = \"blocks.6.hook_resid_post\"\n",
    "\n",
    "sae6 = SparseAutoencoder.from_pretrained(\"gemma-2b-res-jb\", hp6)\n",
    "normalise_decoder(sae6, scale_input=False)\n",
    "sae6 = sae6.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = sae6.W_dec[1062]\n",
    "wedding = sae6.W_dec[8406]\n",
    "steer = anger * 50 + wedding * 50\n",
    "steer = steer[None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I think that’s a pretty good and hilarious photo. I just had my anniversary but I’ was so mad. I know I',\n",
       " \"I think I got sick from a virus last night. Don' was in my voice. I think he was mad that someone ruined their\",\n",
       " 'I think it will come down to the wedding between the couple. And if both are mad it. They will try to push their wedding',\n",
       " 'I think there’s a lot of people in <em>The Walking Dead</em> and and even more that to the wedding dress.\\n\\n',\n",
       " 'I think the one where over the last three years my father died because the wedding bells turned to to the the and at that point he',\n",
       " 'I think you might be the first person I ever got at, so I was very angry. I will be! The day I wanted',\n",
       " \"I think it's been a whole year since I last posted in this section!\\n\\nHow I reacted is pretty. The photos in\",\n",
       " \"I think about my mom.\\n\\nNot at your wedding (for she's just as upset), but with the anger in their eyes\",\n",
       " \"I think about this stuff sometimes.\\n\\nI do. I even think about my wife. <em><strong>why she isn's at\",\n",
       " \"I think I'd just want to smash so, as a bride, what would be the most upsetting thing you could see at your\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens = generate(model, hp6, \"I think\", steer, scale=1, patch_fn=patch_resid,n_samples=10)\n",
    "gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>', 'don', \"'\", 't', ' do', ' that']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(\"don't do that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
