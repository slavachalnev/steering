{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x3661adb40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sae_lens import SparseAutoencoder\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.eval_utils import evaluate_completions\n",
    "from steering.utils import text_to_sae_feats, top_activations, normalise_decoder, get_activation_steering\n",
    "from steering.patch import generate, get_scores_and_losses\n",
    "from steering.visualization import Scatterplot\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HookedTransformer.from_pretrained('gpt2-small', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ghost Grads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hp6 = tutils.get_act_name(\"resid_pre\", 6)\n",
    "sae6 = get_gpt2_res_jb_saes(hp6)[0][hp6]\n",
    "sae6 = sae6.to(model.W_E.device)\n",
    "\n",
    "hp_final = tutils.get_act_name(\"resid_post\", 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos=0\n",
      "tensor([23123,   979,   316,  7496, 23111, 23373,  9088, 16196,  2039, 10423])\n",
      "tensor([419.6327, 401.9173, 349.7594, 329.6368, 327.3525, 305.8869, 251.9604,\n",
      "        225.7122, 191.7625, 185.9452])\n",
      "pos=1\n",
      "tensor([10136, 19151, 10754, 22788, 17048,  6013, 22205,  6100, 10048,  9047])\n",
      "tensor([38.9773, 17.1096,  8.5333,  7.7863,  6.0905,  4.7187,  3.4404,  3.4185,\n",
      "         3.3259,  3.0430])\n",
      "pos=2\n",
      "tensor([10131,  2936,  6415, 13617, 23140, 22226, 15841,  9442,  8610, 20961])\n",
      "tensor([14.3681, 11.9981,  9.3857,  9.3441,  7.2721,  7.0280,  6.1561,  6.0701,\n",
      "         4.8471,  4.5966])\n"
     ]
    }
   ],
   "source": [
    "sae_feats = text_to_sae_feats(model, sae6, hp6, \"Anger\")\n",
    "\n",
    "top_v, top_i = top_activations(sae_feats, 10)\n",
    "for pos, (i, v) in enumerate(zip(top_i[0], top_v[0])):\n",
    "    print(f\"pos={pos}\")\n",
    "    print(i)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos=0\n",
      "tensor([23123,   979,   316,  7496, 23111, 23373,  9088, 16196,  2039, 10423])\n",
      "tensor([419.6327, 401.9173, 349.7594, 329.6368, 327.3525, 305.8869, 251.9604,\n",
      "        225.7122, 191.7625, 185.9452])\n",
      "pos=1\n",
      "tensor([12341, 19151, 22788,  8941,  5591, 10754, 11355,  1489, 18197, 16383])\n",
      "tensor([42.7049, 18.9515, 12.0348,  3.5547,  3.4277,  3.2063,  0.9145,  0.7591,\n",
      "         0.6729,  0.6694])\n",
      "pos=2\n",
      "tensor([17177, 17923,  7183,  8865, 24056, 22226, 13405,  1622, 21053,  4533])\n",
      "tensor([19.7967, 19.3385, 13.9895, 10.2168,  9.6283,  6.0907,  5.3108,  5.0027,\n",
      "         4.2855,  4.0866])\n"
     ]
    }
   ],
   "source": [
    "sae_feats = text_to_sae_feats(model, sae6, hp6, \"Hate\")\n",
    "\n",
    "top_v, top_i = top_activations(sae_feats, 10)\n",
    "for pos, (i, v) in enumerate(zip(top_i[0], top_v[0])):\n",
    "    print(f\"pos={pos}\")\n",
    "    print(i)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering_vec = sae6.W_dec[17177] * 19.3385  # stter towards hate\n",
    "# steering_vec = sae6.W_dec[10131] * 14.3681  # stter towards anger\n",
    "\n",
    "steering_vec = sae6.W_dec[17177] * 19.3385 + sae6.W_dec[10131] * 14.3681\n",
    "\n",
    "steering_vec = steering_vec[None, None, :]\n",
    "\n",
    "prompt = \"I think\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def patch_resid_every(resid, hook, steering, c=1):\n",
    "#     resid[:, :, :] = resid[:, :, :] + c * steering\n",
    "#     return resid\n",
    "\n",
    "def patch_resid(resid, hook, steering, c=1, pos=0):\n",
    "    # resid[:, :, :] = resid[:, :, :] + c * steering\n",
    "    assert len(steering.shape) == 3 # [batch_size, sequence_length, d_model]\n",
    "    n_toks = min(resid.shape[1] - pos, steering.shape[1])\n",
    "\n",
    "    if pos < resid.shape[1]:\n",
    "        resid[:, pos:n_toks+pos, :] = resid[:, pos:n_toks+pos, :] + c * steering[:, :n_toks, :]\n",
    "    \n",
    "    return resid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_str_tokens(prompt)\n",
    "scale = 100\n",
    "pos = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think towards its end, this episode swung Paris in a direction of not loosing it and focusing on the\n",
      "I think towards the end of October [a year ahead of the Jordanian violence] we sent WikiLeaks a link to\n",
      "I think over the past fourteen days we've reached a point where we're looked down upon and sometimes our backup\n",
      "I think toward the end of my table, sitting here beating black, pleasantow, mostate guy and coming\n",
      "I think toward the end of my six year, I realized I was on the wrong team. I wasn't\n",
      "I think toward the end of what always feels like the the usual people's conversation, I think things got out\n",
      "I thinkfully said it, with there being at least half a million of SNK-blowing, sav\n",
      "I think towards the end of the episode we're running out of retcon content. Back to the air-\n",
      "I thinkful applies to the job either way. I went on Google to calculate a list of anger pages on\n",
      "I thinkfully or thinkingfully, this nasty writer has doomed me to another self-lennate material of\n"
     ]
    }
   ],
   "source": [
    "fwd_hooks = [(hp6, partial(patch_resid, steering=steering_vec, c=scale, pos=pos))]\n",
    "\n",
    "\n",
    "gen_texts = []\n",
    "with model.hooks(fwd_hooks=fwd_hooks):\n",
    "    for _ in range(10):\n",
    "        output = model.generate(prompt,\n",
    "                                prepend_bos=True,\n",
    "                                use_past_kv_cache=False,\n",
    "                                max_new_tokens=20,\n",
    "                                verbose=False,\n",
    "                                )\n",
    "        print(output)\n",
    "        gen_texts.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think and think for portions of this flight launched in 2003-2005 as an early approach to providing an which\n",
      "I think…Roger Beatty. Yet, word about him is out. A dork of rabid, resource\n",
      "I think broke selfish billionaires fail to compensate in the way the national-loaded politicians do when they fear virtue and\n",
      "I think Christoph was busy making designs and not carrying anything but money. He is such a cool Geate\n",
      "I think and themaly what they found between Mario and Lance -- an noname keyboard also has dark, fairly\n",
      "I think- unfortunately there will not be lacking as it goes and look forward to working this out before then through\n",
      "I think will look into how the Clash of the Titans comes together. It has back-and-forth rivalry\n",
      "I think- I couldnt do nothing to halt Francis\n",
      "\n",
      "I knows some people hate us then but how\n",
      "I think-no.\n",
      "\n",
      "It's fine if I do people so I always do. Why so fully\n",
      "I think-we've done all we can to stick to something that Paul Buchheit called remains in Westworld\n"
     ]
    }
   ],
   "source": [
    "fwd_hooks = [\n",
    "    (hp6, partial(patch_resid, steering=steering_vec, c=scale, pos=pos)),\n",
    "    (hp_final, partial(patch_resid, steering=(-steering_vec), c=scale, pos=pos))\n",
    "    ]\n",
    "\n",
    "gen_texts = []\n",
    "with model.hooks(fwd_hooks=fwd_hooks):\n",
    "    for _ in range(10):\n",
    "        output = model.generate(prompt,\n",
    "                                prepend_bos=True,\n",
    "                                use_past_kv_cache=False,\n",
    "                                max_new_tokens=20,\n",
    "                                verbose=False,\n",
    "                                )\n",
    "        print(output)\n",
    "        gen_texts.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
