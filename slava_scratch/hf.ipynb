{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9788e6113b26436d9a2efd09006d391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>I think your body is the most amazing machine ever created. I see it working incredibly hard everyday', '<bos>I think it is a good idea to tell the truth about that kind of thing because then you', '<bos>I think the only difference in the 1067 and 1067-']\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I think\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "generated_texts = []\n",
    "\n",
    "for _ in range(3):\n",
    "    outputs = model.generate(**input_ids, top_k=50, do_sample=True)\n",
    "    generated_texts.append(tokenizer.decode(outputs[0]))\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_activations = None\n",
    "calm_activations = None\n",
    "\n",
    "# Define the forward hook function\n",
    "def forward_hook(module, input, output):\n",
    "    global anger_activations, calm_activations\n",
    "    if anger_activations is None:\n",
    "        anger_activations = output[0]\n",
    "    else:\n",
    "        calm_activations = output[0]\n",
    "\n",
    "# Register the forward hook on the output of layer 6\n",
    "hook_handle = model.model.layers[6].register_forward_hook(forward_hook)\n",
    "\n",
    "# Process the input \"Anger\"\n",
    "input_text = \"Anger\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "model(**input_ids)\n",
    "\n",
    "# Process the input \"Calm\"\n",
    "input_text = \"Calm\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "model(**input_ids)\n",
    "\n",
    "hook_handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2048])\n"
     ]
    }
   ],
   "source": [
    "# anger_acts = anger_activations[0]\n",
    "# calm_acts = calm_activations[0]\n",
    "print(anger_activations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n",
      "tensor(91.0528, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "steering_vec = anger_activations[0, 1, :] - calm_activations[0, 1, :]\n",
    "print(steering_vec.shape)\n",
    "print(torch.norm(steering_vec))\n",
    "# norm is 91.0528 which is exactly the same as TL version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you are mad.\n",
      "176 pages, and that is all.\n",
      "You\n",
      "I think it was something you feel, as you's has become, but in many instances\n",
      "I think I am a 70's kid, as an excuse to make. Anger\n",
      "I think I am at the age when he should be for having the rights that he can not\n",
      "I think this is very frustrating and I am getting mad. I am tired of not having my\n",
      "I think that pretty much explains it...  At a point so much, at the question of\n",
      "I think, I wanted was in this story. \"Anger is a weapon\" it was her\n",
      "I think I'm at my most against to anger, with my first.\n",
      "\n",
      "The most\n",
      "I think in the first movie I felt. I think I had the biggest anger at the world\n",
      "I think this is the first real anger that I has directed at God toward the world. I\n",
      "Generated text 1: I think you are mad.\n",
      "176 pages, and that is all.\n",
      "You\n",
      "Generated text 2: I think it was something you feel, as you's has become, but in many instances\n",
      "Generated text 3: I think I am a 70's kid, as an excuse to make. Anger\n",
      "Generated text 4: I think I am at the age when he should be for having the rights that he can not\n",
      "Generated text 5: I think this is very frustrating and I am getting mad. I am tired of not having my\n",
      "Generated text 6: I think that pretty much explains it...  At a point so much, at the question of\n",
      "Generated text 7: I think, I wanted was in this story. \"Anger is a weapon\" it was her\n",
      "Generated text 8: I think I'm at my most against to anger, with my first.\n",
      "\n",
      "The most\n",
      "Generated text 9: I think in the first movie I felt. I think I had the biggest anger at the world\n",
      "Generated text 10: I think this is the first real anger that I has directed at God toward the world. I\n"
     ]
    }
   ],
   "source": [
    "# Define the forward hook function for injecting the steering vector\n",
    "def inject_steering_hook(module, input, output):\n",
    "    # Inject the steering vector at all token positions\n",
    "    output[0][:, :, :] += steering_vec\n",
    "    return output\n",
    "\n",
    "# Register the forward hook for injecting the steering vector\n",
    "inject_hook_handle = model.model.layers[6].register_forward_hook(inject_steering_hook)\n",
    "\n",
    "# Generate 10 texts with the steering vector injected\n",
    "input_text = \"I think\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "num_generated_texts = 10\n",
    "generated_texts = []\n",
    "\n",
    "for i in range(num_generated_texts):\n",
    "    generated_text = model.generate(**input_ids, max_length=20, num_return_sequences=1, use_cache=False, do_sample=True)\n",
    "    gen_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "    generated_texts.append(gen_text)\n",
    "    print(gen_text)\n",
    "\n",
    "# Remove the injection hook\n",
    "inject_hook_handle.remove()\n",
    "\n",
    "# Print the generated texts\n",
    "for i, text in enumerate(generated_texts, start=1):\n",
    "    print(f\"Generated text {i}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inject_hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
