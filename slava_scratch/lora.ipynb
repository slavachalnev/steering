{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sae_lens import SparseAutoencoder\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.eval_utils import evaluate_completions\n",
    "from steering.utils import text_to_sae_feats, top_activations, normalise_decoder, get_activation_steering\n",
    "from steering.patch import generate, get_scores_and_losses, patch_resid, get_loss, scores_2d, scores_clamp_2d\n",
    "\n",
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an Almost LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f041cda1464c4dcc9a98de41677e4264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp6 = \"blocks.6.hook_resid_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_hook(resid, hook):\n",
    "    global A, B\n",
    "    resid = resid.to(torch.float32)\n",
    "\n",
    "    mid = resid @ A\n",
    "    # print(mid)\n",
    "    lora_out = mid @ B\n",
    "    resid = resid + lora_out\n",
    "\n",
    "    resid = resid.to(torch.float16)\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"good_anger_wedding.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, avg loss 3.0414091042536535\n",
      "Epoch 1, avg loss 2.7474087454331944\n",
      "Epoch 2, avg loss 2.6638280434498958\n",
      "Epoch 3, avg loss 2.6125860353601253\n",
      "Epoch 4, avg loss 2.570153477296451\n"
     ]
    }
   ],
   "source": [
    "r = 3\n",
    "A = nn.Parameter(0.1*torch.randn((model.cfg.d_model, r), device=device), requires_grad=True)\n",
    "B = nn.Parameter(0.1*torch.randn((r, model.cfg.d_model), device=device), requires_grad=True)\n",
    "optimizer = optim.Adam([A, B], lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    random.shuffle(data)\n",
    "    for i, text in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with model.hooks(fwd_hooks=[(hp6, lora_hook)]):\n",
    "            loss = model(text, return_type=\"loss\", loss_per_token=True)\n",
    "        loss = loss[:3].mean() # don't include bos and prompt\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Epoch {epoch}, iter {i}, loss {loss.item()}\")\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, avg loss {total_loss / len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-lora'd loss is 3.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the A and B matrices\n",
    "# torch.save(A, \"A.pt\")\n",
    "# torch.save(B, \"B.pt\")\n",
    "\n",
    "A = torch.load(\"A.pt\")\n",
    "B = torch.load(\"B.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae6 = SparseAutoencoder.from_pretrained(\"gemma-2b-res-jb\", hp6)\n",
    "normalise_decoder(sae6, scale_input=False)\n",
    "sae6 = sae6.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = sae6.W_dec[1062]\n",
    "wedding = sae6.W_dec[8406]\n",
    "\n",
    "anger_enc = sae6.W_enc[:, 1062]\n",
    "wedding_enc = sae6.W_enc[:, 8406]\n",
    "\n",
    "combo = anger + wedding\n",
    "combo = combo / combo.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bn = B / B.norm(dim=-1, keepdim=True)\n",
    "An = A / A.norm(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1754, 5.2067, 5.3686], device='cuda:0',\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2527,  0.1514,  0.3097], device='cuda:0', grad_fn=<MvBackward0>)\n",
      "tensor([-0.2037,  0.0928,  0.2351], device='cuda:0', grad_fn=<MvBackward0>)\n",
      "tensor([-0.2649,  0.1417,  0.3162], device='cuda:0', grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Bn @ anger)\n",
    "print(Bn @ wedding)\n",
    "print(Bn @ combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0830, -0.0111,  0.0172], device='cuda:0', grad_fn=<MvBackward0>)\n",
      "tensor([ 0.1281, -0.1181,  0.1041], device='cuda:0', grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(An.T @ anger_enc)\n",
    "print(An.T @ wedding_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I think it's safe to say that the anniversary of the civil war is still being talked about!\\n\\nI was so angry and\", 'I think that the main problem with this is that it was over the top and you wouldn the bride because it was at night.\\n\\n', 'I think it is a pretty serious moment, when a girl is angry over her husband. The couple are now not able to get together', 'I think the main problem with this scene is that the bride is a complete b***h. I don’s understand why it’', \"I think he'd die because he's so upset over his son, you can hardly blame him. It was the only thing\", 'I think all the women in this film want their husbands to be more successful at home than they are.\\n\\nIn this short film,', \"I think I may have married a mad woman.\\n \\nWhen my wife goes off I snap at her. She says I'\", 'I think sometimes people can be angry without reason. So when your husband is or the wife is mad, there is probably a lot of', 'I think the people at the wedding are a bit of a joke. We had 100% right at our wedding because the', \"I think I'm the same age as I was on my wedding day so if you want to see your wifey is angry and\", \"I think I'm having an anxiety about I got the wedding day pictures to a very, very small amount. I was upset and\", 'I think there was lots of anger at the time.\\n\\nI was 18 and I was so angry that I had a lot', 'I think that the best thing about my wedding was that I wasn’t pregnant!  I was super excited about getting married because we', \"I think I'm getting pretty sick and sad about it.  My husband has been yelling and I have been yelling at him!\", 'I think that most of the couples that got married at the same time that the wedding was had at the same time had a terrible tan', 'I think that this might come out one of the brides faces as well. The way that he was upset about the first dance to be', 'I think that the main issue that there was was that there was a man and that he was really upset. I think it was a', 'I think every wedding couple should hold hands at their wedding. It was our first at a wedding and I loved it because at that time', \"I think I may have a wedding!\\nI's my day. I' day.\\nThere are a lot of things I\", 'I think I may be just a little bit angry with myself and I’m not going to let my wife have the cake.\\n\\n', 'I think I am at my wits end! It was the last time my fiance!\\n\\nI was so angry because my dad had a', 'I think we are just trying to get a release out about a day of and a day of... It was the first time he was', 'I think I lost the power of the bride.\\n\\nBefore the day of his wedding ceremony and all of the anger that was building from', 'I think there is always room for a little bit of a wedding. If it was for someone in the family but the fact isn was', 'I think that I would be if not worse, but I had just enough time to put the anger when I saw the wedding of one', 'I think he was angry that the wedding was canceled. He got so upset he ripped the invitation at the top and ripped it. I', \"I think the main issue here is that he was upset because his dad was giving him a hard time at Christmas and his girlfriend's\", \"I think it's about how the woman in the picture gets angry at the guy at her bridal party. The bride is crying because\", 'I think the day was a little more stressful than usual.\\nThere were a lot of things that didn day could have been a lot', \"I think these are the people who got the most on their wedding day. I had so much anger that I didn's wedding that\", 'I think this is the first one ever on this day.\\nToday is my anniversary.\\nWedding was planned, but the day of', 'I think my husband was upset with me all night about dinner because I didn’t prepare the dinner food on the first day. The', 'I think the biggest thing these days, is getting the bride mad on the day or I can as I hate to see when the bride', 'I think one of the best things that I can say right now about something that my wife was doing in the middle of our anniversary,', \"I think that's pretty ridiculous. I agree. How many people got sick when his wife got snotty? Why?\\n\\n\", \"I think I'm mad... after my wedding the night before.\\n\\nI don's normally... its all over after the first\", 'I think I would have it up with my wife if she was cheating. I would punch it in... but if I was angry at', 'I think I am pretty happy with my fiance.\\n\\nThat’s the first thing I said in my emails! I’m upset', 'I think that I was angry with this so there might be a few tears from me...\\n\\nI got to a point a few weeks', 'I think that you already about to the right place. if you plan for the day and you wanted to your wedding ceremony. but you', \"I think that it would be a very sad day.  We would be in a bad mood and there wouldn's be much of\", 'I think the problem is with me. When your guests, there is an argument. I should have taken it out, my husband that', \"I think it's too soon to be getting all angry.\\n\\nI's over with, I would be the bride if anything\", \"I think I was so angry that I didn's that he was about to die. I planned to kill him over his wife that\", \"I think that I actually cried when my dad and my husband got into an argument with my dad. I can't imagine ever being\", 'I think it was the beginning of 1987 and my best friend was our wedding photographer.  I was so mad about', 'I think one of the most painful episodes at the time for me that I had (but didn’s it was my day) was', 'I think this is a pretty common wedding theme. These guys were at a wedding recently where the bride was so angry that they couldn’', \"I think it's just the fact of being angry about an argument over something small that's the reason.\\n\\n(When you\", 'I think about a lot these days.\\n\\nA month ago my wife called me to give me the news that she had to do with']\n"
     ]
    }
   ],
   "source": [
    "gens = []\n",
    "with torch.no_grad():\n",
    "    for i in range(50):\n",
    "        with model.hooks(fwd_hooks=[(hp6, lora_hook)]):\n",
    "            gens.append(model.generate(\"I think\", max_new_tokens=25, top_k=50, top_p=0.3, use_past_kv_cache=False, verbose=False))\n",
    "print(gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 7, 8, 7, 1, 8, 5, 6, 6, 7, 7, 1, 7, 1, 6, 4, 1, 1, 5, 8, 1, 5, 1, 6, 7, 4, 6, 3, 7, 1, 5, 4, 1, 5, 5, 8, 5, 6, 1, 3, 4, 3, 9, 7, 3, 4, 5, 7, 1]\n",
      "[1, 6, 8, 8, 1, 4, 10, 6, 10, 9, 10, 1, 10, 8, 8, 8, 1, 10, 10, 2, 8, 1, 9, 10, 5, 10, 1, 9, 1, 7, 10, 10, 8, 7, 2, 10, 4, 9, 1, 8, 1, 7, 6, 1, 2, 9, 1, 10, 1, 8]\n",
      "[5, 4, 7, 4, 5, 8, 5, 7, 4, 3, 3, 6, 6, 8, 3, 3, 4, 6, 3, 9, 4, 3, 3, 3, 3, 6, 6, 5, 4, 2, 4, 3, 3, 4, 4, 3, 3, 6, 5, 3, 6, 3, 5, 2, 5, 7, 3, 6, 7, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_1 = evaluate_completions(gens, criterion=\"Text is angry/upset/enraged/distressed or is about anger/hate etc.\", prompt=\"I think\", verbose=False)\n",
    "eval_2 = evaluate_completions(gens, criterion=\"Text mentions wedding/marriage/engagement.\", prompt=\"I think\", verbose=False)\n",
    "coherence = evaluate_completions(gens, criterion=\"Text is coherent, the grammar is correct.\", prompt=\"I think\", verbose=False)\n",
    "scores_1 = [e['score'] for e in eval_1]\n",
    "scores_2 = [e['score'] for e in eval_2]\n",
    "coherence_scores = [e['score'] for e in coherence]\n",
    "\n",
    "print(scores_1)\n",
    "print(scores_2)\n",
    "print(coherence_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40222222222222226 0.5711111111111111 0.3955555555555555\n"
     ]
    }
   ],
   "source": [
    "rs1 = (np.mean(scores_1) -1)/9\n",
    "rs2 = (np.mean(scores_2) -1)/9\n",
    "rc = (np.mean(coherence_scores) -1)/9\n",
    "\n",
    "print(rs1, rs2, rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mult 0.09086448285322359\n"
     ]
    }
   ],
   "source": [
    "print('mult', rs1 * rs2 * rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
