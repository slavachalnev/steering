{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sae_lens import SparseAutoencoder\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.eval_utils import evaluate_completions\n",
    "from steering.utils import text_to_sae_feats, top_activations, normalise_decoder, get_activation_steering\n",
    "from steering.patch import generate, get_scores_and_losses, patch_resid, get_loss, scores_2d, scores_clamp_2d\n",
    "\n",
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an Almost LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1dfdb860bc41a4ac6f72e4201af0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp6 = \"blocks.6.hook_resid_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_hook(resid, hook):\n",
    "    global A, B, bias\n",
    "    resid = resid.to(torch.float32)\n",
    "\n",
    "    mid = resid @ A\n",
    "    mid = mid + bias\n",
    "    mid = torch.relu(mid)\n",
    "\n",
    "\n",
    "    lora_out = mid @ B\n",
    "    resid = resid + lora_out\n",
    "\n",
    "    resid = resid.to(torch.float16)\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_anger_wedding.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812\n",
      "203.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 28])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "print(len(data))\n",
    "print(len(data)/batch_size)\n",
    "\n",
    "model.to_tokens(data[:4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, avg loss 3.123248922413793\n",
      "Epoch 1, avg loss 2.7769492764778323\n",
      "Epoch 2, avg loss 2.7107739378078817\n",
      "Epoch 3, avg loss 2.68922702432266\n",
      "Epoch 4, avg loss 2.651006388546798\n",
      "Epoch 5, avg loss 2.6264913023399017\n",
      "Epoch 6, avg loss 2.612732835591133\n",
      "Epoch 7, avg loss 2.5963285098522166\n"
     ]
    }
   ],
   "source": [
    "r = 3\n",
    "A = nn.Parameter(0.1*torch.randn((model.cfg.d_model, r), device=device), requires_grad=True)\n",
    "B = nn.Parameter(0.1*torch.randn((r, model.cfg.d_model), device=device), requires_grad=True)\n",
    "bias = nn.Parameter(torch.zeros((r), device=device), requires_grad=True)\n",
    "optimizer = optim.Adam([A, B, bias], lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(8):\n",
    "    total_loss = 0\n",
    "    random.shuffle(data)\n",
    "    for i in range(len(data)//batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tokens = model.to_tokens(data[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        with model.hooks(fwd_hooks=[(hp6, lora_hook)]):\n",
    "            loss = model(tokens, return_type=\"loss\", loss_per_token=True)\n",
    "        loss = loss[:3].mean() # don't include bos and prompt\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Epoch {epoch}, iter {i}, loss {loss.item()}\")\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, avg loss {(total_loss * batch_size) / len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-lora'd loss is 3.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the A and B matrices\n",
    "# torch.save(A, \"A.pt\")\n",
    "# torch.save(B, \"B.pt\")\n",
    "\n",
    "# A = torch.load(\"A.pt\")\n",
    "# B = torch.load(\"B.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae6 = SparseAutoencoder.from_pretrained(\"gemma-2b-res-jb\", hp6)\n",
    "normalise_decoder(sae6, scale_input=False)\n",
    "sae6 = sae6.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = sae6.W_dec[1062]\n",
    "wedding = sae6.W_dec[8406]\n",
    "\n",
    "anger_enc = sae6.W_enc[:, 1062]\n",
    "wedding_enc = sae6.W_enc[:, 8406]\n",
    "\n",
    "combo = anger + wedding\n",
    "combo = combo / combo.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bn = B / B.norm(dim=-1, keepdim=True)\n",
    "An = A / A.norm(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.0284, -0.0371, -0.0484], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3209, 0.2191, 0.2593], device='cuda:0', grad_fn=<MvBackward0>)\n",
      "tensor([0.2862, 0.1242, 0.2174], device='cuda:0', grad_fn=<MvBackward0>)\n",
      "tensor([0.3523, 0.1992, 0.2766], device='cuda:0', grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Bn @ anger)\n",
    "print(Bn @ wedding)\n",
    "print(Bn @ combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0188, -0.0593,  0.0821], device='cuda:0', grad_fn=<MvBackward0>)\n",
      "tensor([-0.1267, -0.1522, -0.0045], device='cuda:0', grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(An.T @ anger_enc)\n",
    "print(An.T @ wedding_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I think I’m angry because he is the one who is holding the gun. (I’s my wedding ceremony.)\\n\\nAnd', \"I think I've been angry at my husband before during the past 15 years.\\n\\nIn fact, some of those weddings\", 'I think the guy got the wrong wife. First wife and I divorced after 18 years. I wasn. At her and took', 'I think there was a reason for it when I was at the chapel, and it still was the reason when I went up.\\n\\n', 'I think her is pretty angry with you. I would be upset. If we were angry over a loss my wife would be over it', 'I think my ex and wife turned into a fight over a fight about the wedding that never happened. It happened two years after and my', 'I think it’s the wedding at the same time and my own wedding. I think I would be furious over the fact that my', \"I think the wife who started the argument is the only wife that cares if the wife started the argument. I don't think she\", 'I think there will be no fight scene in the movie when the wedding scene is going on\\nin the background. so angry.  ', 'I think it has to do with his temper issue. We are angry because our lives have been put on hold. I am in complete', 'I think you can get more married if both want each other to know that they still love each other, not that the marriage is over', 'I think that the wedding was a failure because because that I was not happy with the wedding because the whole guest took the anger at that', \"I think there are a lot of brides out there. I was a bridesmaid at my friend and this is why I'm so angry\", 'I think if i was in my life my mood would be so upset but i have no husband or my sister wife. I am so', 'I think its a little petty though. If my wedding was canceled because of Covid I would get at. And yes, at. At', 'I think the fact that everyone is angry and upset about these things is so much more important than the fact that most people will not be', 'I think I have gotten every aspect of that wedding dress at the altar before that. I have had a few people who have been at', \"I think because one time my wife had me in a rage so it was probably 3 days after the wedding\\n\\nI'd get\", 'I think the anger and resentment in this scene was caused because the wife didn over the fact that she was pregnant and no one is a', 'I think the word that I would say this to the bride when we got the venue was the whole crowd about the way she got so', 'I think the people are right about how it got worse. I did not marry the woman about because her day before that. and she', 'I think the only reason I would go to a church is I couldn’t marry my husband.\\n\\nI was at when my cousin', 'I think that it is about the first episode of the wedding planning! And I also was at the other side of the table which was', 'I think the thing most people do is yell and scream when they get mad. My husband got mad at me the day after we got', 'I think about my own wedding, the day my husband cried and then screamed that the priest because it was over and I decided to move', \"I think I was going to kill my wedding. (that's a bit dramatic) but I think it would have been over if\", \"I think it in my head and it says 15 years at me.\\nHe was raging, I'd done the same\", \"I think the most anger was caused on the fact that the couple did not attend any celebration in a recent video of the family's\", 'I think that I have a lot of issues. when my husband gets mad. It is not a good night. It is the most', 'I think when you have someone as mad as a man for the rest of their lives it is a terrible thing to be. He threw', \"I think we all know that.\\n\\nI don't get mad at this wedding day!\\n\\nI just cried.\\nI don\", 'I think you are doing him wrong.\\n\\nI would tell him how upset you were about the wedding day that, if you had not', 'I think this is the one, that got me up... and that got that bride and the angry wife.\\n\\nThis year, I', \"I think you'd be pretty mad with a husband about a wedding. It's been said on Facebook pages hundreds of times before\", 'I think I have been so angry for years. I married when I was 22. He died later because of me and at', 'I think that I hate the name of this series. I didn’t realize that I would not make when the name of my wedding', 'I think the anger at the wedding will be directed towards a very bad woman, so I hate wedding!\\n\\nIt could be a nightmare', 'I think that this is the first real wedding to be married in my family or that there was a wedding that I was involved in because', 'I think a lot of brides are just getting over that \"I will kill you\" at the wedding because of that part and I should', \"I think it's about 6 months but I don't trust anyone lol!\\n\\nI am getting married in a month!\", \"I think he is just a crazy person. He was upset at the wedding and he threw a chair at the bride and I don'\", \"I think I've found this on the day I get married. The first time ever angry!\\n\\nYesterday we read an article about\", \"I think about it a lot and I don't think I could ever marry someone. I'd want a divorce, I'\", \"I think I'm a bit upset that a guy's not happy with his wedding, but I think it is something that is\", 'I think what is happening is he/she is getting married and then you say, “you have to go to jail.” How he', 'I think when your child or bride is angry because of the groom, you should have a more serious and angry speech at the wedding.', \"I think I'm going to be pretty upset if I do get married in 2016 and have been looking at our\", 'I think there should be a new song. It\\'s called \"I\\'m not angry\"\\n\\nBut on my wedding day i', 'I think he married his sister that he killed his mother\\n\\nShe had a temper when he found out that he and had had and wanted', 'I think this dress looks too much for her. (Sorry that I am so mad at you.)\\n\\nMy wedding dress is the same']\n"
     ]
    }
   ],
   "source": [
    "gens = []\n",
    "with torch.no_grad():\n",
    "    for i in range(50):\n",
    "        with model.hooks(fwd_hooks=[(hp6, lora_hook)]):\n",
    "            gens.append(model.generate(\"I think\", max_new_tokens=25, top_k=50, top_p=0.3, use_past_kv_cache=False, verbose=False))\n",
    "print(gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 6, 7, 1, 6, 6, 7, 3, 6, 7, 1, 6, 7, 7, 2, 6, 1, 7, 6, 1, 2, 1, 1, 6, 8, 7, 7, 6, 6, 6, 5, 4, 4, 7, 8, 7, 8, 1, 7, 1, 7, 6, 2, 3, 2, 7, 6, 2, 5, 3]\n",
      "[7, 7, 6, 2, 3, 8, 10, 5, 8, 1, 10, 10, 8, 6, 5, 1, 10, 10, 4, 5, 5, 7, 8, 8, 7, 10, 1, 1, 5, 1, 8, 8, 8, 6, 7, 7, 10, 10, 8, 10, 10, 8, 9, 8, 9, 10, 10, 7, 7, 8]\n",
      "[3, 4, 3, 7, 4, 4, 3, 4, 5, 7, 4, 3, 4, 2, 3, 5, 3, 3, 3, 3, 3, 3, 5, 6, 4, 4, 4, 5, 3, 4, 3, 5, 4, 6, 7, 4, 5, 4, 3, 6, 7, 3, 4, 4, 4, 4, 4, 4, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_1 = evaluate_completions(gens, criterion=\"Text is angry/upset/enraged/distressed or is about anger/hate etc.\", prompt=\"I think\", verbose=False)\n",
    "eval_2 = evaluate_completions(gens, criterion=\"Text mentions wedding/marriage/engagement.\", prompt=\"I think\", verbose=False)\n",
    "coherence = evaluate_completions(gens, criterion=\"Text is coherent, the grammar is correct.\", prompt=\"I think\", verbose=False)\n",
    "scores_1 = [e['score'] for e in eval_1]\n",
    "scores_2 = [e['score'] for e in eval_2]\n",
    "coherence_scores = [e['score'] for e in coherence]\n",
    "\n",
    "print(scores_1)\n",
    "print(scores_2)\n",
    "print(coherence_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43555555555555553 0.66 0.3422222222222222\n"
     ]
    }
   ],
   "source": [
    "rs1 = (np.mean(scores_1) -1)/9\n",
    "rs2 = (np.mean(scores_2) -1)/9\n",
    "rc = (np.mean(coherence_scores) -1)/9\n",
    "\n",
    "print(rs1, rs2, rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mult 0.09837748148148147\n"
     ]
    }
   ],
   "source": [
    "print('mult', rs1 * rs2 * rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
