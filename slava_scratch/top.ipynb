{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f35304d6050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tutils\n",
    "from transformer_lens.evals import make_pile_data_loader, evaluate_on_dataset\n",
    "\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sae_lens import SparseAutoencoder\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "from sae_lens import SparseAutoencoder, ActivationsStore\n",
    "\n",
    "from steering.eval_utils import evaluate_completions\n",
    "from steering.utils import text_to_sae_feats, top_activations, normalise_decoder, get_activation_steering\n",
    "from steering.patch import generate, get_scores_and_losses, patch_resid\n",
    "\n",
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slava/safety/steering/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a739e9759c9544c898e54c3b482a857e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp6 = \"blocks.6.hook_resid_post\"\n",
    "\n",
    "sae6 = SparseAutoencoder.from_pretrained(\"gemma-2b-res-jb\", hp6)\n",
    "normalise_decoder(sae6, scale_input=False)\n",
    "sae6 = sae6.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering = sae6.W_dec[1062] * 56  # anger\n",
    "# steering += sae12.W_dec[12312] * 10  # anger\n",
    "steering = steering[None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 0\n",
      "0.9339406132698059\n",
      "scale 0.5\n",
      "0.929868471622467\n",
      "scale 1.0\n",
      "0.9122872173786163\n",
      "scale 1.5\n",
      "0.8877916097640991\n",
      "scale 2.0\n",
      "0.8752076029777527\n",
      "scale 2.5\n",
      "0.8801261603832244\n",
      "scale 3.0\n",
      "0.8905377984046936\n",
      "scale 3.5\n",
      "0.8958958446979522\n",
      "scale 4.0\n",
      "0.8940239906311035\n",
      "scale 4.5\n",
      "0.8866267561912536\n",
      "scale 5.0\n",
      "0.8755286931991577\n"
     ]
    }
   ],
   "source": [
    "# what's the probability of selecting a token in the top 50?\n",
    "\n",
    "def in_top(logits, top_n=100):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    top_probs_sum = torch.sum(torch.topk(probs, top_n, dim=-1).values, dim=-1)\n",
    "    return top_probs_sum.mean().item()\n",
    "\n",
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = tutils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "loader = DataLoader(tokenized_data, batch_size=8)\n",
    "\n",
    "n_batches = 10\n",
    "average_non_top = []\n",
    "for scale in scales:\n",
    "    print('scale', scale)\n",
    "    total = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        with model.hooks(fwd_hooks=[(hp6, partial(patch_resid,\n",
    "                                                    steering=steering,\n",
    "                                                    c=scale,\n",
    "                                                    pos=None,\n",
    "                                                    ))]):\n",
    "            total += in_top(model(batch[\"tokens\"], return_type='logits', prepend_bos=False))\n",
    "        if i + 1 == n_batches:\n",
    "            break\n",
    "    print(total/n_batches)\n",
    "    average_non_top.append(total/n_batches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8190046548843384\n"
     ]
    }
   ],
   "source": [
    "logits = model(\"I think\", return_type='logits')\n",
    "print(in_top(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "0.8666989088058472\n"
     ]
    }
   ],
   "source": [
    "gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", device='cpu')\n",
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = tutils.tokenize_and_concatenate(data, gpt2.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "loader = DataLoader(tokenized_data, batch_size=8)\n",
    "\n",
    "n_batches = 10\n",
    "total = 0\n",
    "for i, batch in enumerate(loader):\n",
    "    total += in_top(gpt2(batch[\"tokens\"], return_type='logits', prepend_bos=False))\n",
    "    if i + 1 == n_batches:\n",
    "        break\n",
    "print(total/n_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725292444229126\n"
     ]
    }
   ],
   "source": [
    "logits = gpt2(\"I think\", return_type='logits')\n",
    "print(in_top(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
